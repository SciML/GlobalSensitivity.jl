var documenterSearchIndex = {"docs":
[{"location":"methods/regression/#Regression-Method","page":"Regression Method","title":"Regression Method","text":"","category":"section"},{"location":"methods/regression/","page":"Regression Method","title":"Regression Method","text":"RegressionGSA","category":"page"},{"location":"methods/regression/#GlobalSensitivity.RegressionGSA","page":"Regression Method","title":"GlobalSensitivity.RegressionGSA","text":"RegressionGSA(; rank::Bool = false)\n\nrank::Bool = false: Flag determining whether to also run a rank regression analysis\n\nProviding this to gsa results in a calculation of the following statistics, provided as a RegressionGSAResult. If the function f to be analyzed is of dimensionality f R^n - R^m, then these coefficients are returned as a matrix, with the corresponding statistic in the (i, j)` entry.\n\npearson: This is equivalent to the correlation coefficient matrix between input and output. The rank version is known as the Spearman coefficient.\nstandard_regression: Standard regression coefficients, also known as sigma-normalized derivatives\npartial_correlation: Partial correlation coefficients, related to the precision matrix and a measure of the correlation of linear models of the\n\nMethod Details\n\nIt is possible to fit a linear model explaining the behavior of Y given the values of X, provided that the sample size n is sufficiently large (at least n > d).\n\nThe measures provided for this analysis by us in GlobalSensitivity.jl are\n\na) Pearson Correlation Coefficient:\n\nr = fracsum_i=1^n (x_i - overlinex)(y_i - overliney)sqrtsum_i=1^n (x_i - overlinex)^2(y_i - overliney)^2\n\nb) Standard Regression Coefficient (SRC):\n\nSRC_j = beta_j sqrtfracVar(X_j)Var(Y)\n\nwhere beta_j is the linear regression coefficient associated to X_j. This is also known as a sigma-normalized derivative.\n\nc) Partial Correlation Coefficient (PCC):\n\nPCC_j = rho(X_j - hatX_-jY_j - hatY_-j)\n\nwhere hatX_-j is the prediction of the linear model, expressing X_j with respect to the other inputs and hatY_-j is the prediction of the linear model where X_j is absent. PCC measures the sensitivity of Y to X_j when the effects of the other inputs have been canceled.\n\nIf rank is set to true, then the rank coefficients are also calculated.\n\nAPI\n\ngsa(f, method::RegressionGSA, p_range::AbstractVector; samples::Int, batch = false)\ngsa(X, Y, method::RegressionGSA)\n\nExample\n\nusing GlobalSensitivity\n\nfunction linear_batch(X)\n    A= 7\n    B= 0.1\n    @. A*X[1,:]+B*X[2,:]\nend\nfunction linear(X)\n    A= 7\n    B= 0.1\n    A*X[1]+B*X[2]\nend\n\np_range = [[-1, 1], [-1, 1]]\nreg = gsa(linear_batch, RegressionGSA(), p_range; batch = true)\n\nreg = gsa(linear, RegressionGSA(), p_range; batch = false)\nreg = gsa(linear, RegressionGSA(true), p_range; batch = false) #with rank coefficients\n\nX = QuasiMonteCarlo.sample(1000, [-1, -1], [1, 1], QuasiMonteCarlo.SobolSample())\nY = reshape(linear.([X[:, i] for i in 1:1000]), 1, 1000)\nreg_mat = gsa(X, Y, RegressionGSA(true))\n\n\n\n\n\n","category":"type"},{"location":"tutorials/parallelized_gsa/#Parallelized-Morris-and-Sobol-Sensitivity-Analysis-of-an-ODE","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"","category":"section"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"Let's run GSA on the Lotka-Volterra model to study the sensitivity of the maximum of predator population and the average prey population.","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"using GlobalSensitivity, Statistics, OrdinaryDiffEq, QuasiMonteCarlo, Plots","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"First, let's define our model:","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"function f(du, u, p, t)\n    du[1] = p[1] * u[1] - p[2] * u[1] * u[2] #prey\n    du[2] = -p[3] * u[2] + p[4] * u[1] * u[2] #predator\nend\nu0 = [1.0; 1.0]\ntspan = (0.0, 10.0)\np = [1.5, 1.0, 3.0, 1.0]\nprob = ODEProblem(f, u0, tspan, p)\nt = collect(range(0, stop = 10, length = 200))","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"Now, let's create a function that takes in a parameter set and calculates the maximum of the predator population and the average of the prey population for those parameter values. To do this, we will make use of the remake function, which creates a new ODEProblem, and use the p keyword argument to set the new parameters:","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"f1 = function (p)\n    prob1 = remake(prob; p = p)\n    sol = solve(prob1, Tsit5(); saveat = t)\n    [mean(sol[1, :]), maximum(sol[2, :])]\nend","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"Now, let's perform a Morris global sensitivity analysis on this model. We specify that the parameter range is [1,5] for each of the parameters, and thus call:","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"m = gsa(f1, Morris(total_num_trajectory = 1000, num_trajectory = 150),\n    [[1, 5], [1, 5], [1, 5], [1, 5]])","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"Let's get the means and variances from the MorrisResult struct.","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"m.means","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"m.variances","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"Let's plot the result","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"scatter(\n    m.means[1, :], m.variances[1, :], series_annotations = [:a, :b, :c, :d], color = :gray)","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"scatter(\n    m.means[2, :], m.variances[2, :], series_annotations = [:a, :b, :c, :d], color = :gray)","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"For the Sobol method, we can similarly do:","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"m = gsa(f1, Sobol(), [[1, 5], [1, 5], [1, 5], [1, 5]], samples = 1000)","category":"page"},{"location":"tutorials/parallelized_gsa/#Direct-Use-of-Design-Matrices","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Direct Use of Design Matrices","text":"","category":"section"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"For the Sobol Method, we can have more control over the sampled points by generating design matrices. Doing it in this manner lets us directly specify a quasi-Monte Carlo sampling method for the parameter space. Here we use QuasiMonteCarlo.jl to generate the design matrices as follows:","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"samples = 500\nlb = [1.0, 1.0, 1.0, 1.0]\nub = [5.0, 5.0, 5.0, 5.0]\nsampler = SobolSample()\nA, B = QuasiMonteCarlo.generate_design_matrices(samples, lb, ub, sampler)","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"and now we tell it to calculate the Sobol indices on these designs for the function f1 we defined in the Lotka-Volterra example:","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"sobol_result = gsa(f1, Sobol(), A, B)","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"We plot the first order and total order Sobol Indices for the parameters (a and b).","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"p1 = bar([\"a\", \"b\", \"c\", \"d\"], sobol_result.ST[1, :],\n    title = \"Total Order Indices prey\", legend = false)\np2 = bar([\"a\", \"b\", \"c\", \"d\"], sobol_result.S1[1, :],\n    title = \"First Order Indices prey\", legend = false)\np1_ = bar([\"a\", \"b\", \"c\", \"d\"], sobol_result.ST[2, :],\n    title = \"Total Order Indices predator\", legend = false)\np2_ = bar([\"a\", \"b\", \"c\", \"d\"], sobol_result.S1[2, :],\n    title = \"First Order Indices predator\", legend = false)\nplot(p1, p2, p1_, p2_)","category":"page"},{"location":"tutorials/parallelized_gsa/#Parallelizing-the-Global-Sensitivity-Analysis","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelizing the Global Sensitivity Analysis","text":"","category":"section"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"In all the previous examples, f(p) was calculated serially. However, we can parallelize our computations by using the batch interface. In the batch interface, each column p[:,i] is a set of parameters, and we output a column for each set of parameters. Here we showcase using the Ensemble Interface to use EnsembleGPUArray to perform automatic multithreaded-parallelization of the ODE solves.","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"function f(du, u, p, t)\n    du[1] = p[1] * u[1] - p[2] * u[1] * u[2] #prey\n    du[2] = -p[3] * u[2] + p[4] * u[1] * u[2] #predator\nend\n\nu0 = [1.0; 1.0]\ntspan = (0.0, 10.0)\np = [1.5, 1.0, 3.0, 1.0]\nprob = ODEProblem(f, u0, tspan, p)\nt = collect(range(0, stop = 10, length = 200))\n\nf1 = function (p)\n    prob_func(prob, i, repeat) = remake(prob; p = p[:, i])\n    ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n    sol = solve(\n        ensemble_prob, Tsit5(), EnsembleThreads(); saveat = t, trajectories = size(p, 2))\n    # Now sol[i] is the solution for the ith set of parameters\n    out = zeros(2, size(p, 2))\n    for i in 1:size(p, 2)\n        out[1, i] = mean(sol[i][1, :])\n        out[2, i] = maximum(sol[i][2, :])\n    end\n    out\nend","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"And now to do the parallelized calls, we simply add the batch=true keyword argument:","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"sobol_result = gsa(f1, Sobol(), A, B, batch = true)","category":"page"},{"location":"tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"This user-side parallelism thus allows you to take control, and thus for example you can use DiffEqGPU.jl for automated GPU-parallelism of the ODE-based global sensitivity analysis!","category":"page"},{"location":"methods/fractional/#Fractional-Factorial-Method","page":"Fractional Factorial Method","title":"Fractional Factorial Method","text":"","category":"section"},{"location":"methods/fractional/","page":"Fractional Factorial Method","title":"Fractional Factorial Method","text":"FractionalFactorial","category":"page"},{"location":"methods/fractional/#GlobalSensitivity.FractionalFactorial","page":"Fractional Factorial Method","title":"GlobalSensitivity.FractionalFactorial","text":"FractionalFactorial()\n\nFractionalFactorial does not have any keyword arguments.\n\nMethod Details\n\nFractional Factorial method creates a design matrix by utilizing Hadamard Matrix and uses it to run simulations of the input model. The main effects are then evaluated by dot product between the contrast for the parameter and the vector of simulation results. The corresponding main effects and variance, i.e. square of the main effects, are returned as results for Fractional Factorial method.\n\nAPI\n\ngsa(f, method::FractionalFactorial; num_params, p_range = nothing, kwargs...)\n\nExample\n\nusing GlobalSensitivity, Test\n\nf = X -> X[1] + 2 * X[2] + 3 * X[3] + 4 * X[7] * X[12]\nres1 = gsa(f,FractionalFactorial(),num_params = 12,samples=10)\n\n\n\n\n\n","category":"type"},{"location":"methods/shapley/#Shapley-Method","page":"Shapley Method","title":"Shapley Method","text":"","category":"section"},{"location":"methods/shapley/","page":"Shapley Method","title":"Shapley Method","text":"Shapley","category":"page"},{"location":"methods/shapley/#GlobalSensitivity.Shapley","page":"Shapley Method","title":"GlobalSensitivity.Shapley","text":"Shapley(n_perms, n_var, n_outer, n_inner)\n\nn_perms: number of permutations to consider. Defaults to -1, which means all permutations           are considered hence the exact Shapley effects           algorithm is used. If n_perms is set to a positive integer, then the random version           of Shapley effects is used.\nn_var: size of each bootstrapped sample\nn_outer: number of samples to be taken to estimate conditional variance\nn_inner: size of each n_outer sample taken\n\nMethod Details\n\nShapely effects is a variance based method to assign attribution to each feature based on how sentitive the function to the feature. Shapley effects take into account that features could be dependent, which is not possible in previous methods like Sobol indices. In our implementation, we use Copulas.jl to define the joint input distribution as a SklarDist.\n\nAPI\n\ngsa(f, method::Shapley, input_distribution::SklarDist; batch=false)\n\nExample\n\nusing Copulas, Distributions, GlobalSensitivity\n\nfunction ishi(X)\n    A = 7\n    B = 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\nfunction ishi_batch(X)\n    A = 7\n    B = 0.1\n    @. sin(X[1, :]) + A*sin(X[2, :])^2+ B*X[3, :]^4 *sin(X[1, :])\nend\n\nn_perms = -1; # -1 indicates that we want to consider all permutations. One can also use n_perms > 0\nn_var = 1000;\nn_outer = 100;\nn_inner = 3\n\ndim = 3;\nmargins = (Uniform(-pi, pi), Uniform(-pi, pi), Uniform(-pi, pi));\ndependency_matrix = Matrix(I, dim, dim)\n\nC = GaussianCopula(dependency_matrix);\ninput_distribution = SklarDist(C,margins);\n\nmethod = Shapley(n_perms=n_perms, n_var = n_var, n_outer = n_outer, n_inner = n_inner);\n\n###### non-batch\nresult_non_batch = gsa(ishi,method,input_distribution,batch=false)\nshapley_effects = result_non_batch.shapley_effects\nprintln(shapley_effects)\n\n###### batch\nresult_batch = gsa(ishi_batch,method,input_distribution,batch=true)\nshapley_effects = result_batch.shapley_effects\nprintln(shapley_effects)\n\n#### Example with correlated inputs\nd = 3\nmu = zeros(d)\nsig = [1, 1, 2]\nro = 0.9\nCormat = [1 0 0; 0 1 ro; 0 ro 1]\nCovmat = (sig * transpose(sig)) .* Cormat\n\nmargins = [Normal(mu[i], sig[i]) for i in 1:d]\ncopula = GaussianCopula((sig * transpose(sig)) .* Cormat)\ninput_distribution = SklarDist(copula, margins)\n\nresult = gsa(ishi, method, input_distribution, batch = false)\n\n\n\n\n\n","category":"type"},{"location":"methods/easi/#EASI-Method","page":"EASI Method","title":"EASI Method","text":"","category":"section"},{"location":"methods/easi/","page":"EASI Method","title":"EASI Method","text":"EASI","category":"page"},{"location":"methods/easi/#GlobalSensitivity.EASI","page":"EASI Method","title":"GlobalSensitivity.EASI","text":"EASI(; max_harmonic::Int = 10, dct_method::Bool = false)\n\nmax_harmonic: Maximum harmonic of the input frequency for which the output power spectrum is analyzed for. Defaults to 10.\ndct_method: Use Discrete Cosine Transform method to compute the power spectrum. Defaults to false.\n\nMethod Details\n\nThe EASI method is a Fourier-based technique for performing variance-based methods of global sensitivity analysis for the computation of first order effects (Sobol' indices), hence belonging into the same class of algorithms as FAST and RBD. It is a computationally cheap method for which existing data can be used. Unlike the FAST and RBD methods which use a specially generated sample set that contains suitable frequency data for the input factors, in EASI these frequencies are introduced by sorting and shuffling the available input samples.\n\nAPI\n\ngsa(f, method::EASI, p_range; samples, batch = false)\ngsa(X, Y, method::EASI)\n\nExample\n\nusing GlobalSensitivity, Test\n\nfunction ishi_batch(X)\n    A= 7\n    B= 0.1\n    @. sin(X[1,:]) + A*sin(X[2,:])^2+ B*X[3,:]^4 *sin(X[1,:])\nend\nfunction ishi(X)\n    A= 7\n    B= 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\nlb = -ones(4)*π\nub = ones(4)*π\n\nres1 = gsa(ishi,EASI(),[[lb[i],ub[i]] for i in 1:4],samples=15000)\nres2 = gsa(ishi_batch,EASI(),[[lb[i],ub[i]] for i in 1:4],samples=15000,batch=true)\n\nX = QuasiMonteCarlo.sample(15000, lb, ub, QuasiMonteCarlo.SobolSample())\nY = ishi.([X[:, i] for i in 1:15000])\n\nres1 = gsa(X, Y, EASI())\nres1 = gsa(X, Y, EASI(; dct_method = true))\n\n\n\n\n\n","category":"type"},{"location":"methods/efast/#eFAST-Method","page":"eFAST Method","title":"eFAST Method","text":"","category":"section"},{"location":"methods/efast/","page":"eFAST Method","title":"eFAST Method","text":"eFAST","category":"page"},{"location":"methods/efast/#GlobalSensitivity.eFAST","page":"eFAST Method","title":"GlobalSensitivity.eFAST","text":"eFAST(; num_harmonics::Int = 4)\n\nnum_harmonics: the number of harmonics to sum in the Fourier series decomposition, this defaults to 4.\n\nMethod Details\n\neFAST offers a robust, especially at low sample size, and computationally efficient procedure to get the first and total order indices as discussed in Sobol. It utilizes monodimensional Fourier decomposition along a curve, exploring the parameter space. The curve is defined by a set of parametric equations,\n\nx_i(s) = G_i(sin ω_is)  i=12  N\n\nwhere s is a scalar variable varying over the range -  s  +, G_i are transformation functions and ω_i  i=12N is a set of different (angular) frequencies, to be properly selected, associated with each factor for all N (samples) number of parameter sets. For more details, on the transformation used and other implementation details you can go through  A. Saltelli et al..\n\nAPI\n\ngsa(f, method::eFAST, p_range::AbstractVector; samples::Int, batch = false,\n         distributed::Val{SHARED_ARRAY} = Val(false),\n         rng::AbstractRNG = Random.default_rng(), kwargs...) where {SHARED_ARRAY}\n\nNote, p_range is either a vector of tuples for the upper and lower bound or a vector of Distributions. \n\nExample\n\nBelow we show use of eFAST on the Ishigami function.\n\nusing GlobalSensitivity, QuasiMonteCarlo, Distributions\n\nfunction ishi(X)\n    A= 7\n    B= 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\n## define upper and lower limits, a.k.a uniform distributions\nlb = -ones(4)*π\nub = ones(4)*π\n\nres1 = gsa(ishi, eFAST(), [[lb[i],ub[i]] for i in 1:4], samples=15000)\n\n# define distributions for the inputs\ninput_ranges = [Normal(0, 1),\n                Uniform(-π, π),\n                Uniform(-π, π),\n                Uniform(-π, π)]\n\nres2 = gsa(ishi, eFAST(), input_ranges, samples=15000)\n\n## with batching\nfunction ishi_batch(X)\n    A= 7\n    B= 0.1\n    @. sin(X[1,:]) + A*sin(X[2,:])^2+ B*X[3,:]^4 *sin(X[1,:])\nend\n\nres3 = gsa(ishi_batch, eFAST(), [[lb[i],ub[i]] for i in 1:4], samples=15000, batch=true)\n\n\n\n\n\n","category":"type"},{"location":"methods/morris/#Morris-Method","page":"Morris Method","title":"Morris Method","text":"","category":"section"},{"location":"methods/morris/","page":"Morris Method","title":"Morris Method","text":"Morris","category":"page"},{"location":"methods/morris/#GlobalSensitivity.Morris","page":"Morris Method","title":"GlobalSensitivity.Morris","text":"Morris(; p_steps::Array{Int, 1} = Int[], relative_scale::Bool = false,\n            num_trajectory::Int = 10,\n            total_num_trajectory::Int = 5 * num_trajectory, len_design_mat::Int = 10)\n\np_steps: Vector of Delta for the step sizes in each direction. Required.\nrelative_scale: The elementary effects are calculated with the assumption that the parameters lie in the range [0,1] but as this is not always the case scaling is used to get more informative, scaled effects. Defaults to false.\ntotal_num_trajectory, num_trajectory: The total number of design matrices that are generated, out of which num_trajectory matrices with the highest spread are used in calculation.\nlen_design_mat: The size of a design matrix.\n\nMethod Details\n\nThe Morris method also known as Morris’s OAT method where OAT stands for One At a Time can be described in the following steps:\n\nWe calculate local sensitivity measures known as “elementary effects”, which are calculated by measuring the perturbation in the output of the model on changing one parameter.\n\nEE_i = fracf(x_1x_2x_i+ Deltax_k) - yDelta\n\nThese are evaluated at various points in the input chosen such that a wide “spread” of the parameter space is explored and considered in the analysis, to provide an approximate global importance measure. The mean and variance of these elementary effects is computed. A high value of the mean implies that a parameter is important, a high variance implies that its effects are non-linear or the result of interactions with other inputs. This method does not evaluate separately the contribution from the interaction and the contribution of the parameters individually and gives the effects for each parameter which takes into consideration all the interactions and its individual contribution.\n\nAPI\n\ngsa(f, method::Morris, p_range::AbstractVector; batch = false,\n         rng::AbstractRNG = Random.default_rng(), kwargs...)\n\nExample\n\nMorris method on Ishigami function\n\nusing GlobalSensitivity\n\nfunction ishi(X)\n    A= 7\n    B= 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\nlb = -ones(4)*π\nub = ones(4)*π\n\nm = gsa(ishi, Morris(num_trajectory=500000), [[lb[i],ub[i]] for i in 1:4])\n\n\n\n\n\n","category":"type"},{"location":"tutorials/shapley/#Using-the-Shapley-method-in-case-of-correlated-inputs","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"","category":"section"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"One of the primary drawbacks of typical global sensitivity analysis methods is their inability to handle correlated inputs. The Shapley method is one of the few methods that can handle correlated inputs. The Shapley method is a game-theoretic approach that is based on the idea of marginal contributions of each input to the output.","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"It has gained extensive popularity in the field of machine learning and is used to explain the predictions of black-box models. Here we will use the Shapley method on a Scientific Machine Learning (SciML) model to understand the impact of each parameter on the output.","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"We will use a Neural ODE trained on a simulated dataset from the Spiral ODE model. The Neural ODE is trained to predict output at a given time. The Neural ODE is trained using the SciML ecosystem.","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"As the first step let's generate the dataset.","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"using GlobalSensitivity, OrdinaryDiffEq, Flux, SciMLSensitivity, LinearAlgebra\nusing Optimization, OptimizationOptimisers, Distributions, Copulas, CairoMakie\n\nu0 = [2.0f0; 0.0f0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1f0 2.0f0; -2.0f0 -0.1f0]\n    du .= ((u .^ 3)'true_A)'\nend\nt = range(tspan[1], tspan[2], length = datasize)\nprob = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob, Tsit5(), saveat = t))","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"Now we will define our Neural Network for the dynamics of the system. We will use a 2-layer neural network with 10 hidden units in the first layer and the second layer. We will use the Chain function from Flux to define our NN. A detailed tutorial on is available here.","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"dudt2 = Flux.Chain(x -> x .^ 3,\n    Flux.Dense(2, 10, tanh),\n    Flux.Dense(10, 2))\np, re = Flux.destructure(dudt2) # use this p as the initial condition!\ndudt(u, p, t) = re(p)(u) # need to restrcture for backprop!\nprob = ODEProblem(dudt, u0, tspan)\n\nθ = [u0; p] # the parameter vector to optimize\n\nfunction predict_n_ode(θ)\n    Array(solve(prob, Tsit5(), u0 = θ[1:2], p = θ[3:end], saveat = t))\nend\n\nfunction loss_n_ode(θ)\n    pred = predict_n_ode(θ)\n    loss = sum(abs2, ode_data .- pred)\n    loss\nend\n\nloss_n_ode(θ)\n\ncallback = function (state, l) #callback function to observe training\n    display(l)\n    return false\nend\n\n# Display the ODE with the initial parameter values.\ncallback(θ, loss_n_ode(θ))\n\n# use Optimization.jl to solve the problem\nadtype = Optimization.AutoZygote()\n\noptf = Optimization.OptimizationFunction((p, _) -> loss_n_ode(p), adtype)\noptprob = Optimization.OptimizationProblem(optf, θ)\n\nresult_neuralode = Optimization.solve(optprob,\n    OptimizationOptimisers.Adam(0.05),\n    callback = callback,\n    maxiters = 300)","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"Now we will use the Shapley method to understand the impact of each parameter on the resultant of the cost function. We will use the Shapley function from GlobalSensitivity to compute the so called Shapley Effects. We will first have to define some distributions for the parameters. We will use the standard Normal distribution for all the parameters.","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"First let's assume no correlation between the parameters. Hence the covariance matrix is passed as the identity matrix.","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"d = length(θ)\nmu = zeros(Float32, d)\n#covariance matrix for the copula\nCovmat = Matrix(1.0f0 * I, d, d)\n#the marginal distributions for each parameter\nmarginals = [Normal(mu[i]) for i in 1:d]\n\ncopula = GaussianCopula(Covmat)\ninput_distribution = SklarDist(copula, marginals)\n\nfunction batched_loss_n_ode(θ)\n    # The copula returns samples of `Float64`s\n    θ = convert(AbstractArray{Float32}, θ)\n    prob_func(prob, i, repeat) = remake(prob; u0 = θ[1:2, i], p = θ[3:end, i])\n    ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n    sol = solve(\n        ensemble_prob, Tsit5(), EnsembleThreads(); saveat = t, trajectories = size(θ, 2))\n    out = zeros(size(θ, 2))\n    for i in 1:size(θ, 2)\n        out[i] = sum(abs2, ode_data .- sol[i])\n    end\n    return out\nend\n\nshapley_effects = gsa(\n    batched_loss_n_ode, Shapley(; n_perms = 100, n_var = 100, n_outer = 10),\n    input_distribution, batch = true)","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"barplot(\n    1:54, shapley_effects.shapley_effects;\n    color = :green,\n    figure = (; size = (600, 400)),\n    axis = (;\n        xlabel = \"parameters\",\n        xticklabelrotation = 1,\n        xticks = (1:54, [\"θ$i\" for i in 1:54]),\n        ylabel = \"Shapley Indices\",\n        limits = (nothing, (0.0, 0.2))\n    )\n)","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"Now let's assume some correlation between the parameters. We will use a correlation of 0.09 between all the parameters.","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"Corrmat = fill(0.09f0, d, d)\nfor i in 1:d\n    Corrmat[i, i] = 1.0f0\nend\n\n#since the marginals are standard normal the covariance matrix and correlation matrix are the same\ncopula = GaussianCopula(Corrmat)\ninput_distribution = SklarDist(copula, marginals)\nshapley_effects = gsa(\n    batched_loss_n_ode, Shapley(; n_perms = 100, n_var = 100, n_outer = 100),\n    input_distribution, batch = true)","category":"page"},{"location":"tutorials/shapley/","page":"Using the Shapley method in case of correlated inputs","title":"Using the Shapley method in case of correlated inputs","text":"barplot(\n    1:54, shapley_effects.shapley_effects;\n    color = :green,\n    figure = (; size = (600, 400)),\n    axis = (;\n        xlabel = \"parameters\",\n        xticklabelrotation = 1,\n        xticks = (1:54, [\"θ$i\" for i in 1:54]),\n        ylabel = \"Shapley Indices\",\n        limits = (nothing, (0.0, 0.2))\n    )\n)","category":"page"},{"location":"methods/sobol/#Sobol-Method","page":"Sobol Method","title":"Sobol Method","text":"","category":"section"},{"location":"methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":"Sobol","category":"page"},{"location":"methods/sobol/#GlobalSensitivity.Sobol","page":"Sobol Method","title":"GlobalSensitivity.Sobol","text":"Sobol(; order = [0, 1], nboot = 1, conf_level = 0.95)\n\norder: the order of the indices to calculate. Defaults to [0,1], which means the Total and First order indices. Passing 2 enables calculation of the Second order indices as well.\nnboot: for confidence interval calculation nboot should be specified for the number (>0) of bootstrap runs.\nconf_level: the confidence level, the default for which is 0.95.\n\nMethod Details\n\nSobol is a variance-based method, and it decomposes the variance of the output of the model or system into fractions which can be attributed to inputs or sets of inputs. This helps to get not just the individual parameter's sensitivities, but also gives a way to quantify the affect and sensitivity from the interaction between the parameters.\n\n Y = f_0+ sum_i=1^d f_i(X_i)+ sum_i  j^d f_ij(X_iX_j)  + f_12d(X_1X_2X_d)\n\n Var(Y) = sum_i=1^d V_i + sum_i  j^d V_ij +  + V_12d\n\nThe Sobol Indices are \"order\"ed, the first order indices given by S_i = fracV_iVar(Y) the contribution to the output variance of the main effect of X_i. Therefore, it measures the effect of varying X_i alone, but averaged over variations in other input parameters. It is standardized by the total variance to provide a fractional contribution. Higher-order interaction indices S_ij S_ijk and so on can be formed by dividing other terms in the variance decomposition by Var(Y).\n\nAPI\n\ngsa(f, method::Sobol, p_range::AbstractVector; samples, kwargs...)\ngsa(f, method::Sobol, A::AbstractMatrix{TA}, B::AbstractMatrix;\n         batch = false, Ei_estimator = :Jansen1999,\n         distributed::Val{SHARED_ARRAY} = Val(false),\n         kwargs...) where {TA, SHARED_ARRAY}\n\nEi_estimator can take :Homma1996, :Sobol2007 and :Jansen1999 for which   Monte Carlo estimator is used for the Ei term. Defaults to :Jansen1999. Details for these can be found in the   corresponding papers:\n\n:Homma1996 - Homma, T. and Saltelli, A., 1996. Importance measures in global sensitivity analysis of nonlinear models. Reliability Engineering & System Safety, 52(1), pp.1-17.\n:Sobol2007 - I.M. Sobol, S. Tarantola, D. Gatelli, S.S. Kucherenko and W. Mauntz, 2007, Estimating the approx- imation errors when fixing unessential factors in global sensitivity analysis, Reliability Engineering and System Safety, 92, 957–960. and A. Saltelli, P. Annoni, I. Azzini, F. Campolongo, M. Ratto and S. Tarantola, 2010, Variance based sensitivity analysis of model output. Design and estimator for the total sensitivity index, Computer Physics Communications 181, 259–270.\n:Jansen1999 - M.J.W. Jansen, 1999, Analysis of variance designs for model output, Computer Physics Communi- cation, 117, 35–43.\n:Janon2014 - Janon, A., Klein, T., Lagnoux, A., Nodet, M., & Prieur, C. (2014). Asymptotic normality and efficiency of two Sobol index estimators. ESAIM: Probability and Statistics, 18, 342-364.\n\nExample\n\nusing GlobalSensitivity, QuasiMonteCarlo\n\nfunction ishi(X)\n    A= 7\n    B= 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\nsamples = 600000\nlb = -ones(4)*π\nub = ones(4)*π\nsampler = SobolSample()\nA,B = QuasiMonteCarlo.generate_design_matrices(samples,lb,ub,sampler)\n\nres1 = gsa(ishi,Sobol(order=[0,1,2]),A,B)\n\nfunction ishi_batch(X)\n    A= 7\n    B= 0.1\n    @. sin(X[1,:]) + A*sin(X[2,:])^2+ B*X[3,:]^4 *sin(X[1,:])\nend\n\nres2 = gsa(ishi_batch,Sobol(),A,B,batch=true)\n\n\n\n\n\n","category":"type"},{"location":"tutorials/juliacon21/#Global-Sensitivity-Analysis-of-the-Lotka-Volterra-model","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"","category":"section"},{"location":"tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"The tutorial covers a workflow of using GlobalSensitivity.jl on the Lotka-Volterra differential equations. We showcase how to use multiple GSA methods, analyze their results and leverage Julia's parallelism capabilities to perform Global Sensitivity analysis at scale.","category":"page"},{"location":"tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"using GlobalSensitivity, QuasiMonteCarlo, OrdinaryDiffEq, Statistics, CairoMakie\n\nfunction f(du, u, p, t)\n    du[1] = p[1] * u[1] - p[2] * u[1] * u[2] #prey\n    du[2] = -p[3] * u[2] + p[4] * u[1] * u[2] #predator\nend\n\nu0 = [1.0; 1.0]\ntspan = (0.0, 10.0)\np = [1.5, 1.0, 3.0, 1.0]\nprob = ODEProblem(f, u0, tspan, p)\nt = collect(range(0, stop = 10, length = 200))\n\nf1 = function (p)\n    prob1 = remake(prob; p = p)\n    sol = solve(prob1, Tsit5(); saveat = t)\n    return [mean(sol[1, :]), maximum(sol[2, :])]\nend\n\nbounds = [[1, 5], [1, 5], [1, 5], [1, 5]]\n\nreg_sens = gsa(f1, RegressionGSA(true), bounds, samples = 200)\nfig = Figure(resolution = (600, 400))\nax, hm = CairoMakie.heatmap(fig[1, 1],\n    reg_sens.partial_correlation,\n    axis = (xticksvisible = false, yticksvisible = false, yticklabelsvisible = false,\n        xticklabelsvisible = false, title = \"Partial correlation\"))\nColorbar(fig[1, 2], hm)\nax, hm = CairoMakie.heatmap(fig[2, 1],\n    reg_sens.standard_regression,\n    axis = (xticksvisible = false, yticksvisible = false, yticklabelsvisible = false,\n        xticklabelsvisible = false, title = \"Standard regression\"))\nColorbar(fig[2, 2], hm)\nfig","category":"page"},{"location":"tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"using StableRNGs\n_rng = StableRNG(1234)\nmorris_sens = gsa(f1, Morris(), bounds, rng = _rng)\nfig = Figure(resolution = (600, 400))\nCairoMakie.scatter(fig[1, 1], [1, 2, 3, 4], morris_sens.means_star[1, :], color = :green,\n    axis = (xticksvisible = false, xticklabelsvisible = false, title = \"Prey\"))\nCairoMakie.scatter(fig[1, 2], [1, 2, 3, 4], morris_sens.means_star[2, :], color = :red,\n    axis = (xticksvisible = false, xticklabelsvisible = false, title = \"Predator\"))\nfig","category":"page"},{"location":"tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"sobol_sens = gsa(f1, Sobol(), bounds, samples = 500)\nefast_sens = gsa(f1, eFAST(), bounds, samples = 500)","category":"page"},{"location":"tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"fig = Figure(resolution = (600, 400))\nbarplot(fig[1, 1],\n    [1, 2, 3, 4],\n    sobol_sens.S1[1, :],\n    color = :green,\n    axis = (xticksvisible = false, xticklabelsvisible = false,\n        title = \"Prey (Sobol)\", ylabel = \"First order\"))\nbarplot(fig[2, 1], [1, 2, 3, 4], sobol_sens.ST[1, :], color = :green,\n    axis = (xticksvisible = false, xticklabelsvisible = false, ylabel = \"Total order\"))\nbarplot(fig[1, 2], [1, 2, 3, 4], efast_sens.S1[1, :], color = :red,\n    axis = (xticksvisible = false, xticklabelsvisible = false, title = \"Prey (eFAST)\"))\nbarplot(fig[2, 2], [1, 2, 3, 4], efast_sens.ST[1, :], color = :red,\n    axis = (xticksvisible = false, xticklabelsvisible = false))\nfig","category":"page"},{"location":"tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"fig = Figure(resolution = (600, 400))\nbarplot(fig[1, 1],\n    [1, 2, 3, 4],\n    sobol_sens.S1[2, :],\n    color = :green,\n    axis = (xticksvisible = false, xticklabelsvisible = false,\n        title = \"Predator (Sobol)\", ylabel = \"First order\"))\nbarplot(fig[2, 1], [1, 2, 3, 4], sobol_sens.ST[2, :], color = :green,\n    axis = (xticksvisible = false, xticklabelsvisible = false, ylabel = \"Total order\"))\nbarplot(fig[1, 2], [1, 2, 3, 4], efast_sens.S1[2, :], color = :red,\n    axis = (xticksvisible = false, xticklabelsvisible = false, title = \"Predator (eFAST)\"))\nbarplot(fig[2, 2], [1, 2, 3, 4], efast_sens.ST[2, :], color = :red,\n    axis = (xticksvisible = false, xticklabelsvisible = false))\nfig","category":"page"},{"location":"tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"using QuasiMonteCarlo\nsamples = 500\nlb = [1.0, 1.0, 1.0, 1.0]\nub = [5.0, 5.0, 5.0, 5.0]\nsampler = SobolSample()\nA, B = QuasiMonteCarlo.generate_design_matrices(samples, lb, ub, sampler)\nsobol_sens_desmat = gsa(f1, Sobol(), A, B)\n\nf_batch = function (p)\n    prob_func(prob, i, repeat) = remake(prob; p = p[:, i])\n    ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n\n    sol = solve(\n        ensemble_prob, Tsit5(), EnsembleThreads(); saveat = t, trajectories = size(p, 2))\n\n    out = zeros(2, size(p, 2))\n\n    for i in 1:size(p, 2)\n        out[1, i] = mean(sol[i][1, :])\n        out[2, i] = maximum(sol[i][2, :])\n    end\n\n    return out\nend\n\nsobol_sens_batch = gsa(f_batch, Sobol(), A, B, batch = true)\n\n@time gsa(f1, Sobol(), A, B)\n@time gsa(f_batch, Sobol(), A, B, batch = true)","category":"page"},{"location":"tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"f1 = function (p)\n    prob1 = remake(prob; p = p)\n    sol = solve(prob1, Tsit5(); saveat = t)\nend\nsobol_sens = gsa(f1, Sobol(nboot = 20), bounds, samples = 500)\nfig = Figure(resolution = (600, 400))\nax, hm = CairoMakie.scatter(\n    fig[1, 1], sobol_sens.S1[1][1, 2:end], label = \"Prey\", markersize = 4)\nCairoMakie.scatter!(\n    fig[1, 1], sobol_sens.S1[1][2, 2:end], label = \"Predator\", markersize = 4)\n\n# Legend(fig[1,2], ax)\n\nax, hm = CairoMakie.scatter(\n    fig[1, 2], sobol_sens.S1[2][1, 2:end], label = \"Prey\", markersize = 4)\nCairoMakie.scatter!(\n    fig[1, 2], sobol_sens.S1[2][2, 2:end], label = \"Predator\", markersize = 4)\n\nax, hm = CairoMakie.scatter(\n    fig[2, 1], sobol_sens.S1[3][1, 2:end], label = \"Prey\", markersize = 4)\nCairoMakie.scatter!(\n    fig[2, 1], sobol_sens.S1[3][2, 2:end], label = \"Predator\", markersize = 4)\n\nax, hm = CairoMakie.scatter(\n    fig[2, 2], sobol_sens.S1[4][1, 2:end], label = \"Prey\", markersize = 4)\nCairoMakie.scatter!(\n    fig[2, 2], sobol_sens.S1[4][2, 2:end], label = \"Predator\", markersize = 4)\n\ntitle = Label(fig[0, :], \"First order Sobol indices\")\nlegend = Legend(fig[2, 3], ax)\nfig","category":"page"},{"location":"methods/rbdfast/#Random-Balance-Design-FAST-Method","page":"Random Balance Design FAST Method","title":"Random Balance Design FAST Method","text":"","category":"section"},{"location":"methods/rbdfast/","page":"Random Balance Design FAST Method","title":"Random Balance Design FAST Method","text":"RBDFAST","category":"page"},{"location":"methods/rbdfast/#GlobalSensitivity.RBDFAST","page":"Random Balance Design FAST Method","title":"GlobalSensitivity.RBDFAST","text":"RBDFAST(; num_harmonics = 6)\n\nnum_harmonics: Number of harmonics to consider during power spectral density analysis.\n\nMethod Details\n\nIn the Random Balance Designs (RBD) method, similar to eFAST,  samples points are selected over a curve in the input space. A fixed frequency equal to 1 is used for each factor. Then independent random permutations are applied to the coordinates of the samples points in order to generate the design points. The input model for analysis is evaluated at each design point, and the outputs are reordered such that the design points are in increasing order with respect to factor Xi. The Fourier spectrum is calculated on the model output at the frequency 1 and at its higher harmonics (2, 3, 4, 5, 6) and yields the estimate of the sensitivity index of factor Xi.\n\nAPI\n\ngsa(f, method::RBDFAST; num_params, samples,\n         rng::AbstractRNG = Random.default_rng(), batch = false, kwargs...)\n\nExample\n\nfunction linear_batch(X)\n    A= 7\n    B= 0.1\n    @. A*X[1,:]+B*X[2,:]\nend\nfunction linear(X)\n    A= 7\n    B= 0.1\n    A*X[1]+B*X[2]\nend\n\nlb = -ones(4)*π\nub = ones(4)*π\n\nrng = StableRNG(123)\nres1 = gsa(linear,GlobalSensitivity.RBDFAST(),num_params = 4, samples=15000)\nres2 = gsa(linear_batch,GlobalSensitivity.RBDFAST(),num_params = 4, batch=true, samples=15000)\n\n\n\n\n\n","category":"type"},{"location":"methods/rsa/#RSA-Method","page":"RSA Method","title":"RSA Method","text":"","category":"section"},{"location":"methods/rsa/","page":"RSA Method","title":"RSA Method","text":"RSA","category":"page"},{"location":"methods/rsa/#GlobalSensitivity.RSA","page":"RSA Method","title":"GlobalSensitivity.RSA","text":"RSA(; n_dummy_parameters::Int = 10, acceptance_threshold::Union{Function, Real} = mean)\n\nn_dummy_parameters: Number of dummy parameters to add to the model, used for sensitivity hypothesis testing and to check the amount of samples. Defaults to 10.\nacceptance_threshold: Threshold or function to compute the threshold for defining the acceptance distribution of the sensitivity outputs. The function must be of signature f(Y)   and return a real number, where Y is the output of given sensitivity criterion. Defaults to the mean of the sensitivity values.  \n\nMethod Details\n\nThe RSA (Regional Sensitivity Analysis) method[1] is a monte-carlo based technique for performing nonparametric global sensitivity analysis. The method is based on the Kolmogorov-Smirnov (KS) test, which is a nonparametric test of the equality of continuous,  one-dimensional probability distributions. Each result of a monte-carlo simulation is either classified as behavior (B) or non-behavior (barB). For each parameter i,  the cumulative distributions of the behavior and non-behavior outputs are determined as F_i and barF_i, respectively. The sensitivity for each parameter i is then given by:\n\nS_i = sup_j F_ij - (1 - F_ij)\n\nwhere F_ij is sample j of the cumulative distribution of the sensitivity output for parameter i.\n\nDummy parameters are added to the model to check the amount of samples and to perform sensitivity hypothesis testing. Note that because of random sampling the results may vary between runs. \n\nAPI\n\ngsa(f, method::RSA, p_range; samples, batch = false)\n\nReturns a RSAResult object containing the sensitivity indices for the parameters as S, and mean and standard deviation of the dummy parameters as a tuple Sd = (<mean>, <std>).\n\nExample\n\nusing GlobalSensitivity\n\nfunction ishi_batch(X)\n    A= 7\n    B= 0.1\n    @. sin(X[1,:]) + A*sin(X[2,:])^2+ B*X[3,:]^4 *sin(X[1,:])\nend\nfunction ishi(X)\n    A= 7\n    B= 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\nlb = -ones(4)*π\nub = ones(4)*π\n\nres1 = gsa(ishi,RSA(),[[lb[i],ub[i]] for i in 1:4],samples=15000)\nres2 = gsa(ishi_batch,RSA(),[[lb[i],ub[i]] for i in 1:4],samples=15000,batch=true)\n\nReferences\n\n[1]: Hornberger, G.M. & Spear, Robert. (1981). An Approach to the Preliminary Analysis of Environmental Systems. J. Environ. Manage.; (United States). 12:1. \n\n\n\n\n\n","category":"type"},{"location":"methods/dgsm/#Derivative-based-Global-Sensitivity-Measure-Method","page":"Derivative based Global Sensitivity Measure Method","title":"Derivative based Global Sensitivity Measure Method","text":"","category":"section"},{"location":"methods/dgsm/","page":"Derivative based Global Sensitivity Measure Method","title":"Derivative based Global Sensitivity Measure Method","text":"DGSM","category":"page"},{"location":"methods/dgsm/#GlobalSensitivity.DGSM","page":"Derivative based Global Sensitivity Measure Method","title":"GlobalSensitivity.DGSM","text":"DGSM(; crossed::Bool = false)\n\ncrossed: A Boolean which act as indicator for computation of DGSM crossed indices.\n\nMethod Details\n\nThe DGSM method takes a probability distribution for each of the parameters, and samples are obtained from the distributions to create random parameter sets. Derivatives of the function being analyzed are then computed at the sampled parameters and specific statistics of those derivatives are used. The paper by Sobol and Kucherenko discusses the relationship between the DGSM results, tao and sigma and the Morris elementary effects and Sobol Indices.\n\nAPI\n\ngsa(f, method::DGSM, distr::AbstractArray; samples::Int, kwargs...)\n\ndist: Array of distribution of respective variables. E.g. dist = [Normal(5,6),Uniform(2,3)] for two variables.\n\nExample\n\nusing GlobalSensitivity, Test, Distributions\n\nsamples = 2000000\n\nf1(x) = x[1] + 2*x[2] + 6.00*x[3]\ndist1 = [Uniform(4,10),Normal(4,23),Beta(2,3)]\nb =  gsa(f1,DGSM(),dist1,samples=samples)\n\n\n\n\n\n","category":"type"},{"location":"methods/delta/#Delta-Moment-Independent-Method","page":"Delta Moment-Independent Method","title":"Delta Moment-Independent Method","text":"","category":"section"},{"location":"methods/delta/","page":"Delta Moment-Independent Method","title":"Delta Moment-Independent Method","text":"DeltaMoment","category":"page"},{"location":"methods/delta/#GlobalSensitivity.DeltaMoment","page":"Delta Moment-Independent Method","title":"GlobalSensitivity.DeltaMoment","text":"DeltaMoment(; nboot = 500, conf_level = 0.95, Ygrid_length = 2048,\n                 num_classes = nothing)\n\nnboot: number of bootstrap repetitions. Defaults to 500.\nconf_level: the level used for confidence interval calculation with bootstrap. Default value of 0.95.\nYgrid_length: number of quadrature points to consider when performing the kernel density estimation and the integration steps. Should be a power of 2 for efficient FFT in kernel density estimates. Defaults to 2048.\nnum_classes: Determine how many classes to split each factor into to when generating distributions of model output conditioned on class.\n\nMethod Details\n\nThe Delta moment-independent method relies on new estimators for density-based statistics.  It allows for the estimation of both distribution-based sensitivity measures and of sensitivity measures that look at contributions to a specific moment. One of the primary advantage of this method is the independence of computation cost from the number of parameters.\n\nnote: Note\nDeltaMoment only works for scalar output.\n\nAPI\n\ngsa(f, method::DeltaMoment, p_range; samples, batch = false,\n         rng::AbstractRNG = Random.default_rng())\ngsa(X, Y, method::DeltaMoment; rng::AbstractRNG = Random.default_rng())\n\nExample\n\nusing GlobalSensitivity, Test\n\nfunction ishi(X)\n    A= 7\n    B= 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\nlb = -ones(3)*π\nub = ones(3)*π\n\nm = gsa(ishi,DeltaMoment(),fill([lb[1], ub[1]], 3), samples=1000)\n\n\nsamples = 1000\nX = QuasiMonteCarlo.sample(samples, lb, ub, QuasiMonteCarlo.SobolSample())\nY = ishi.(@view X[:, i] for i in 1:samples)\n\nm = gsa(X, Y, DeltaMoment())\n\n\n\n\n\n","category":"type"},{"location":"#GlobalSensitivity.jl:-Robust,-Fast,-and-Parallel-Global-Sensitivity-Analysis-(GSA)-in-Julia","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Robust, Fast, and Parallel Global Sensitivity Analysis (GSA) in Julia","text":"","category":"section"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"Global Sensitivity Analysis (GSA) methods are used to quantify the uncertainty in the output of a model with respect to the parameters. These methods allow practitioners to measure both parameter's individual contributions and the contribution of their interactions to the output uncertainty.","category":"page"},{"location":"#Installation","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"Installation","text":"","category":"section"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"To use this functionality, you must install GlobalSensitivity.jl:","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"using Pkg\nPkg.add(\"GlobalSensitivity\")","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"Note: GlobalSensitivity.jl is unrelated to the GlobalSensitivityAnalysis.jl package.","category":"page"},{"location":"#General-Interface","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"General Interface","text":"","category":"section"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"The general interface for performing global sensitivity analysis using this package is:","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"gsa(f, method::GlobalSensitivity.GSAMethod, param_range; samples, batch = false)","category":"page"},{"location":"#GlobalSensitivity.gsa-Tuple{Any, GlobalSensitivity.GSAMethod, Any}","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.gsa","text":"gsa(f, method::GSAMethod, param_range; samples, batch=false)\n\nwhere:\n\ny=f(x) is a function that takes in a single vector and spits out a single vector or scalar. If batch=true, then f takes in a matrix where each row is a set of parameters, and returns a matrix where each row is the output for the corresponding row of parameters.\nmethod is one of the available GSA methods.\nparam_range is a vector of tuples for the upper and lower bound for the given parameter i.\nsamples is a required keyword argument for the number of samples of parameters for the design matrix. Note that this is not relevant for Fractional Factorial Method and Morris Method.\n\nAdditionally,\n\nFor Delta Moment-Independent Method, EASI Method and Regression Method input and output matrix-based method as follows is available:\n\nres = gsa(X, Y, method)\n\nwhere:\n\nX is the number of parameters * samples matrix with parameter values.\nY is the output dimension * number of samples matrix with are evaluated at X's columns.\nmethod is one of the GSA methods below.\n\nFor Sobol Method, one can use the following design matrices-based method instead of parameter range-based method discussed earlier:\n\neffects = gsa(f, method, A, B; batch=false)\n\nwhere A and B are design matrices, with each row being a set of parameters. Note that generate_design_matrices from QuasiMonteCarlo.jl can be used to generate the design matrices.\n\n\n\n\n\n","category":"method"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"The descriptions of the available methods can be found in the Methods section. The gsa interface allows for utilizing batched functions with the batch kwarg discussed above for parallel computation of GSA results.","category":"page"},{"location":"#Citing","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"Citing","text":"","category":"section"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"If you use this software in your work, please cite:","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"@article{dixit2022globalsensitivity,\n  title={GlobalSensitivity. jl: Performant and Parallel Global Sensitivity Analysis with Julia},\n  author={Dixit, Vaibhav Kumar and Rackauckas, Christopher},\n  journal={Journal of Open Source Software},\n  volume={7},\n  number={76},\n  pages={4561},\n  year={2022}\n}","category":"page"},{"location":"#Reproducibility","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"</details>","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"</details>","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"</details>","category":"page"},{"location":"","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"}]
}
